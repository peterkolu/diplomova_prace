import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import itertools
import matplotlib.pyplot as plt

# Training data
df = pd.read_csv("C:\\Users\\lucip\\Desktop\\diplomová práce\\vysledky\\agb_corine_onehot\\AGB_training_all_epsg25833.csv")

# Predictors selection
features = ['NDVI', 'NDVI705', 'MSI', 'SAVI', 'elevation', 'slope',
            'forest_type_2018_1', 'forest_type_2018_2', 'forest_type_2018_3']
target = 'AGB'


df = df.dropna(subset=features + [target])
X = df[features].astype(float)
y = df[target]

# Parameters for model
param_grid = {
    'n_estimators': [100, 300],
    'max_depth': [3, 4, 6],
    'learning_rate': [0.03, 0.1],
    'subsample': [0.7, 1],
    'colsample_bytree': [0.7, 1]
}

# Combination of parameters
param_combinations = list(itertools.product(*param_grid.values()))
param_names = list(param_grid.keys())

results = []
kfold = KFold(n_splits=5, shuffle=True, random_state=42)

for i, values in enumerate(param_combinations):
    params = dict(zip(param_names, values))
    model = xgb.XGBRegressor(**params, random_state=42, verbosity=0)

    rmse_scores, mae_scores, r2_scores = [], [], []

    for train_idx, val_idx in kfold.split(X):
        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

        model.fit(X_train, y_train)
        y_pred = model.predict(X_val)

        rmse_scores.append(np.sqrt(mean_squared_error(y_val, y_pred)))
        mae_scores.append(mean_absolute_error(y_val, y_pred))
        r2_scores.append(r2_score(y_val, y_pred))

    results.append({
        'Model_ID': f'M{i+1}',
        'RMSE': np.mean(rmse_scores),
        'MAE': np.mean(mae_scores),
        'R2': np.mean(r2_scores),
        **params  
    })

# Results
results_df = pd.DataFrame(results).sort_values(by='R2', ascending=False)

# Top models
print("TOP 10 modelů s parametry:")
print(results_df.head(10)[['Model_ID', 'RMSE', 'MAE', 'R2',
                           'n_estimators', 'max_depth', 'learning_rate',
                           'subsample', 'colsample_bytree']])


top = results_df.head(10)
labels = top['Model_ID']
x = np.arange(len(labels))
width = 0.25

fig, ax = plt.subplots(figsize=(12, 6))
ax.bar(x - width, top['RMSE'], width, label='RMSE')
ax.bar(x, top['MAE'], width, label='MAE', color="orange")
ax.bar(x + width, top['R2'], width, label='R²', color="green")
ax.set_xticks(x)
ax.set_xticklabels(labels)
ax.set_title("Porovnání XGBoost modelů (TOP 10)")
ax.set_ylabel("Score")
ax.legend()
plt.tight_layout()
plt.show()
