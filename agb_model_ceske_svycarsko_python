import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, make_scorer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
import shap
import rasterio
import matplotlib.pyplot as plt

# training data
df = pd.read_csv("C:\\Users\\lucip\\Desktop\\diplomová práce\\vysledky\\agb_corine_onehot\\AGB_training_data_10m.csv")

# selected predictors
features = ['NDVI', 'NDVI705', 'MSI', 'elevation', 'slope']
target = 'AGB'

# removing NaN
df = df.dropna(subset=features + [target])


# x and y definition
X = df[features].astype(float)
y = df[target]

X = X.astype(float)

# cross-validation(5-fold)
model = xgb.XGBRegressor(
    n_estimators=300,
    max_depth=3,
    learning_rate=0.03,
    subsample=1.0,
    colsample_bytree=0.7,
    random_state=42
)
kfold = KFold(n_splits=5, shuffle=True, random_state=42)

neg_mse = cross_val_score(model, X, y, scoring=make_scorer(mean_squared_error, greater_is_better=False), cv=kfold)
rmse_scores = np.sqrt(-neg_mse)
mae_scores = -cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=kfold)
r2_scores = cross_val_score(model, X, y, scoring='r2', cv=kfold)

# statistics
print(f"cross-validation (5-fold):")
print(f"  RMSE: {rmse_scores.mean():.2f}")
print(f"  MAE: {mae_scores.mean():.2f}")
print(f"  R²: {r2_scores.mean():.3f}")

# statistics visualization
fig, ax = plt.subplots(1, 3, figsize=(15, 4))
ax[0].bar(range(1, 6), rmse_scores)
ax[0].set_title("RMSE per fold")
ax[1].bar(range(1, 6), mae_scores, color="orange")
ax[1].set_title("MAE per fold")
ax[2].bar(range(1, 6), r2_scores, color="green")
ax[2].set_title("R² per fold")
plt.tight_layout()
plt.show()

# training and testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)
print(f"testing:")
print(f"  RMSE: {rmse:.2f}")
print(f"  R²: {r2:.3f}")

# SHAP analysis
explainer = shap.Explainer(model, X_train)
shap_values = explainer(X_test)


shap_abs = np.abs(shap_values.values)
mean_shap = np.mean(shap_abs, axis=0)
shap_importance = pd.Series(mean_shap, index=X_test.columns)
shap_weights = shap_importance / shap_importance.sum()

print("Normalized SHAP weights of predictors :")
for var, weight in shap_weights.items():
    print(f"  {var}: {weight:.4f}")

# weighted sum of predictors
X_weighted_sum = X_test.mul(shap_weights).sum(axis=1)

# 8) linear regression: AGB = α * sum + β
linreg = LinearRegression()
linreg.fit(X_weighted_sum.values.reshape(-1, 1), y_test.values)

# output of coefficients
alpha = linreg.coef_[0]
beta = linreg.intercept_
print("Linear equation for GEE calculation of AGB:")
print(f"  AGB = {alpha:.4f} * (weighted sum of predictors) + {beta:.4f}")
